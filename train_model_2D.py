import numpy as np
import pandas as pd
import torch
from leaspy import AlgorithmSettings, Leaspy
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import torchvision.transforms as transforms
import argparse
import os

from dataset.Dataset2D import Dataset2D
from longitudinalModel.fit_longitudinal_estimator_on_nn import fit_longitudinal_estimator_on_nn
from nnModels.CVAE2D import CVAE2D
from longitudinalModel.train import train
from dataset.group_based_train_test_split import group_based_train_test_split

from nnModels.CVAE2D_ORIGINAL import CVAE2D_ORIGINAL
from nnModels.losses import spatial_auto_encoder_loss

from utils_display.display_individual_observations_2D import display_individual_observations_2D
from dataset.LongitudinalDataset2D import LongitudinalDataset2D, longitudinal_collate_2D
from nnModels.train_AE import train_AE
"""
Script to train the full model. Neural network model + longitudinal estimator
"""
parser = argparse.ArgumentParser()
parser.add_argument('--data', type=str, required=False, default="./data_csv/starmen_dataset.csv",
                    help='csv file path')
parser.add_argument('--nnmodel_name', type=str, required=False, default='CVAE2D',
                    help='Name of the NN model that will be used')
parser.add_argument('--gamma', type=float, required=False, default=100,
                    help='hyperparameter gamma value used for computing the loss')
parser.add_argument('--beta', type=float, required=False, default=5,
                    help='hyperparameter beta value used for computing the loss, default = 5')
parser.add_argument('--dimension', type=int, required=False, default=4,
                    help='size of the latent representation generated by the neural network encoder, default =4')
parser.add_argument('--iterations', type=int, required=False, default=200,
                    help='Number of iterations when training the longitudinal estimator, default = 200')
parser.add_argument('--lr', type=float, required=False, default=1e-4,
                    help='Learning rate to train the VAE, default = 1e-4')
parser.add_argument('--batch_size', type=int, required=False, default=256,
                    help='batch_size to train the VAE, default = 256')
parser.add_argument('-f', '--freeze', type=str, required=False, default='y',
                    help='freeze convolution layer ? default = y')
temp_args, _ = parser.parse_known_args()

freeze_path = "freeze_conv" if temp_args.freeze == 'y' else "no_freeze"

parser.add_argument('--nnmodel_path', type=str, required=False,
                    default=f'saved_models_2D/{freeze_path}/{temp_args.nnmodel_name}_{temp_args.dimension}_{temp_args.beta}_{temp_args.gamma}_{temp_args.iterations}.pth',
                    help='path where the neural network model parameters are saved')
parser.add_argument('--longitudinal_estimator_path', type=str, required=False,
                    default=f'saved_models_2D/{freeze_path}/longitudinal_estimator_params_{temp_args.nnmodel_name}_{temp_args.dimension}_{temp_args.beta}_{temp_args.gamma}_{temp_args.iterations}.json',
                    help='path where the longitudinal estimator parameters are saved')
args = parser.parse_args()

# First we get the different train/validation/test dataset
df = pd.read_csv(args.data)
if not(os.path.isfile("data_csv/starmen_train_set.csv")) and not(os.path.isfile("data_csv/starmen_test_set.csv")) and not(os.path.isfile("data_csv/starmen_validation_set.csv")):
    # Split the data into train (80%) and test (20%) sets
    train_val_df, test_df = group_based_train_test_split(df, test_size=0.2, group_col='subject_id', random_state=42)
    train_df, validation_df = group_based_train_test_split(train_val_df, test_size=0.125, group_col='subject_id', random_state=42)
    # Save the training set to a CSV file
    train_df.to_csv('data_csv/starmen_train_set.csv', index=False)
    validation_df.to_csv('data_csv/starmen_validation_set.csv', index=False)
    # Save the test set to a CSV file
    test_df.to_csv('data_csv/starmen_test_set.csv', index=False)

###  Hyperparameters of the Variational autoencoder model
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print("device = ", device)
num_worker = round(os.cpu_count()/6)   # For faster GPU training

batch_size = args.batch_size
latent_representation_size = args.dimension
gamma = args.gamma
beta = args.beta
initial_lr = args.lr
nn_saving_path = args.nnmodel_path
longitudinal_saving_path = args.longitudinal_estimator_path
# HERE TO CHANGE VAE ARCHITECTURE
model = CVAE2D_ORIGINAL(latent_representation_size)
model.gamma = gamma
model.beta = beta
loss_function = spatial_auto_encoder_loss
print(f"{args.nnmodel_name}_{beta}_{gamma}_{latent_representation_size}_{args.iterations}")

### Hyperparameters of the longitudinal estimator
test_saem_estimator = Leaspy("linear", noise_model="gaussian_diagonal", source_dimension=latent_representation_size - 1)
all_losses = []
algo_settings = AlgorithmSettings('mcmc_saem', n_iter=args.iterations, seed=45, noise_model="gaussian_diagonal")
# algo_settings.set_logs(
#     path=f'outputs/longitudinal_{temp_args.nnmodel_name}_{temp_args.dimension}_{temp_args.beta}_{temp_args.gamma}/logs',  # Creates a logs file ; if existing, ask if rewrite it
#     save_periodicity=50,  # Saves the values in csv files every N iterations
#     console_print_periodicity=1000,  # Displays logs in the console/terminal every N iterations, or None
#     plot_periodicity=1000,  # Generates the convergence plots every N iterations
#     overwrite_logs_folder=True  # if True and the logs folder already exists, it entirely overwrites it
# )

algo_final_fitting_settings = AlgorithmSettings('mcmc_saem', n_iter=10000, seed=45, noise_model="gaussian_diagonal")

# Preparation of the data
transformations = transforms.Compose([])


def open_npy(path):
    return torch.from_numpy(np.load(path)).float()


# If we want to remove existing checkpoints
# folder_path = 'saved_models_2D'
# for file_name in os.listdir(folder_path):
#     file_path = os.path.join(folder_path, file_name)
#     os.unlink(file_path)


output_path = f"training_plots/{args.nnmodel_name}_{temp_args.dimension}_{temp_args.beta}_{temp_args.gamma}_{args.iterations}/"
os.makedirs(os.path.dirname(output_path), exist_ok=True)


# Training of the vanilla VAE
validation_dataset = Dataset2D('data_csv/starmen_validation_set.csv', read_image=open_npy,
                                   transform=transformations)
easy_dataset = Dataset2D('data_csv/starmen_train_set.csv', read_image=open_npy,
                         transform=transformations)

data_loader = DataLoader(easy_dataset, batch_size=batch_size, num_workers=num_worker, shuffle=True, pin_memory=True, )
validation_data_loader = DataLoader(validation_dataset, batch_size=batch_size, num_workers=num_worker, shuffle=True,
                                    pin_memory=True, )

all_losses, _ = train_AE(model, data_loader, nb_epochs=300, device=device,
                         nn_saving_path=nn_saving_path,
                         loss_graph_saving_path=None, spatial_loss=loss_function,
                         validation_data_loader=validation_data_loader)


model.to(device)
plt.plot(np.arange(1, len(all_losses) + 1), all_losses, label="Train loss (VAE)")
plt.legend()
plt.grid(True)
plt.savefig(f"{output_path}loss_VAE.pdf")
plt.show()



# Training of the Longitudinal VAE
model.load_state_dict(torch.load(nn_saving_path, map_location='cpu'))
if args.freeze == "y":
    model.freeze_conv()
best_loss = 1e15
validation_dataset = LongitudinalDataset2D('data_csv/starmen_validation_set.csv', read_image=open_npy,
                                           transform=transformations)
easy_dataset = LongitudinalDataset2D('data_csv/starmen_train_set.csv', read_image=open_npy,
                                     transform=transformations)

data_loader = DataLoader(easy_dataset, batch_size=batch_size, num_workers=num_worker, shuffle=False,
                         collate_fn=longitudinal_collate_2D)
validation_data_loader = DataLoader(validation_dataset, batch_size=batch_size, num_workers=num_worker, shuffle=False,
                                    collate_fn=longitudinal_collate_2D)
best_loss, lvae_losses = train(model, data_loader, test_saem_estimator, algo_settings, nb_epochs=300,
                          lr=initial_lr,
                          nn_saving_path=nn_saving_path + f"2", longitudinal_saving_path=longitudinal_saving_path,
                          loss_graph_saving_path=f"{output_path}/loss_longitudinal_only.pdf", previous_best_loss=best_loss,
                          spatial_loss=loss_function, validation_data_loader=validation_data_loader)
test_saem_estimator = Leaspy.load(longitudinal_saving_path)
model.load_state_dict(torch.load(nn_saving_path + "2", map_location='cpu'))

plt.plot(np.arange(len(all_losses), len(all_losses) + len(lvae_losses)), lvae_losses, label="Train loss (LVAE)")
plt.grid(True)
plt.legend()
plt.savefig(f"{output_path}loss_LVAE.pdf")
plt.show()


# Using the trained LVAE to do some projection
algo_settings = AlgorithmSettings('mcmc_saem', n_iter=30000, seed=45, noise_model="gaussian_diagonal")
results_estimator, _ = fit_longitudinal_estimator_on_nn(data_loader, model, device, test_saem_estimator,
                                                        algo_settings)
results_estimator.save(longitudinal_saving_path + "2")

display_individual_observations_2D(model, 9, './data_csv/starmen_dataset.csv',
                                           fitted_longitudinal_estimator=results_estimator,
                                           save_path=f"{output_path}results_2D_subject9_proj.pdf")
display_individual_observations_2D(model, 9, './data_csv/starmen_dataset.csv',
                                           fitted_longitudinal_estimator=None,
                                           save_path=f"{output_path}results_2D_subject9_noproj.pdf")
