import numpy as np
import pandas as pd
import torch
from leaspy import AlgorithmSettings, Leaspy
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import torchvision.transforms as transforms
import argparse
import os

from dataset.Dataset2D import Dataset2D_patch
from longitudinalModel.fit_longitudinal_estimator_on_nn import fit_longitudinal_estimator_on_nn
# from nnModels.CVAE2D import CVAE2D
from longitudinalModel.train import train, train_kfold_patch, train_kfold_patch_v1, train_kfold_patch_v2
from dataset.group_based_train_test_split import group_based_train_test_split
from dataset.split_k_folds import train_k_folds_split_patch

from nnModels.CVAE2D_PATCH import CVAE2D_PATCH, CVAE2D_PATCH_16, CVAE2D_PATCH_32, CVAE2D_PATCH_3latent64, CVAE2D_PATCH_3latent32, CVAE2D_PATCH_7
from nnModels.losses import spatial_auto_encoder_loss

from utils.display_individual_observations_2D import display_individual_observations_2D
from utils.loading_image import open_npy
from dataset.LongitudinalDataset2D_patch import LongitudinalDataset2D_patch, longitudinal_collate_2D_patch
from nnModels.train_AE import train_AE_kfold
from train.cross_validation import CV_VAE, CV_LVAE
"""
Script to train the full model. Neural network model + longitudinal estimator
"""
parser = argparse.ArgumentParser()
parser.add_argument('--data', type=str, required=False, default="./data_csv/starmen_dataset_patch.csv",
                    help='csv file path')
parser.add_argument('--nnmodel_name', type=str, required=False, default='CVAE2D',
                    help='Name of the NN model that will be used')
parser.add_argument('--gamma', type=float, required=False, default=5,
                    help='hyperparameter gamma value used for computing the loss')
parser.add_argument('--beta', type=float, required=False, default=0.5,
                    help='hyperparameter beta value used for computing the loss, default = 5')
parser.add_argument('--dimension', type=int, required=False, default=64,
                    help='size of the latent representation generated by the neural network encoder, default =4')
parser.add_argument('--iterations', type=int, required=False, default=200,
                    help='Number of iterations when training the longitudinal estimator, default = 200')
parser.add_argument('--lr', type=float, required=False, default=1e-4,
                    help='Learning rate to train the VAE, default = 1e-4')
parser.add_argument('--batch_size', type=int, required=False, default=5,
                    help='batch_size to train the VAE, default = 5')
parser.add_argument("-skip", type=str, required=False, default="n")
args = parser.parse_args()

# First we get the different train/validation/test dataset
df = pd.read_csv(args.data)


train_val_df, test_df = group_based_train_test_split(df, test_size=0.2, group_col='subject_id', random_state=42)
test_df.to_csv('data_csv/starmen_patch_test_set.csv', index=False)
folds_index = train_k_folds_split_patch(train_val_df, 8)

###  Hyperparameters of the Variational autoencoder model
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print("device = ", device)
num_worker = round(os.cpu_count()/2)   # For faster GPU training

batch_size = args.batch_size
latent_representation_size = args.dimension
gamma = args.gamma
beta = args.beta
initial_lr = args.lr
loss_function = spatial_auto_encoder_loss
print(f"{args.nnmodel_name}_{latent_representation_size}_{beta}_{gamma}_{args.iterations}")

if latent_representation_size == 332:
    model_type = CVAE2D_PATCH_3latent32
elif latent_representation_size == 32:
    model_type = CVAE2D_PATCH_32
elif latent_representation_size == 364:
    model_type = CVAE2D_PATCH_3latent64
elif latent_representation_size == 7:
    model_type = CVAE2D_PATCH_7
else:
    model_type = CVAE2D_PATCH_16



output_path = f"plots/training_plots/VAE_patch_folds/patch_{args.nnmodel_name}_{latent_representation_size}_{beta}/"
os.makedirs(os.path.dirname(output_path), exist_ok=True)

# Training of the vanilla VAE
VAE_saving_path = f"saved_models_2D/VAE_patch_folds/{args.nnmodel_name}_{latent_representation_size}_{beta}"
os.makedirs(os.path.dirname(f"saved_models_2D/VAE_patch_folds/"), exist_ok=True)
path_best_fold_model = f"saved_models_2D/best_patch_fold_{args.nnmodel_name}_{latent_representation_size}_{beta}.pth"

if args.skip == "n":
    train_AE_kfold(model_type, folds_index, nb_epochs=80, device=device,
                nn_saving_path=VAE_saving_path,
                loss_graph_saving_path=output_path, spatial_loss=loss_function,
                batch_size=batch_size, num_workers=num_worker,
                latent_dimension=latent_representation_size, gamma=gamma, beta=beta, train_patch=True)
    
    best_fold = CV_VAE(model_type, folds_index, test_df, VAE_saving_path, plot_save_path=output_path,
                    latent_dimension=latent_representation_size, gamma=gamma, beta=beta,
                    batch_size=batch_size, num_worker=num_worker, cv_patch=True)

    model = model_type(latent_representation_size)
    model.gamma = gamma
    model.beta = beta
    model.load_state_dict(torch.load(VAE_saving_path+f"_fold_{best_fold}.pth", map_location='cpu'))
    torch.save(model.state_dict(), path_best_fold_model)

# Training of the Longitudinal VAE
output_path = f"plots/training_plots/LVAE_folds/{args.nnmodel_name}_{latent_representation_size}_{beta}_{gamma}_{args.iterations}/"
os.makedirs(os.path.dirname(output_path), exist_ok=True)

### Hyperparameters of the longitudinal estimator
algo_settings = AlgorithmSettings('mcmc_saem', n_iter=args.iterations, seed=45, noise_model="gaussian_diagonal", device=device)


LVAE_saving_path = f"saved_models_2D/LVAE_folds/patch_{args.nnmodel_name}_{latent_representation_size}_{beta}_{gamma}_{args.iterations}"
longitudinal_saving_path = f'saved_models_2D/LVAE_folds/patch_longitudinal_estimator_params_{args.nnmodel_name}_{latent_representation_size}_{beta}_{gamma}_{args.iterations}'
os.makedirs(os.path.dirname(LVAE_saving_path), exist_ok=True)
os.makedirs(os.path.dirname(longitudinal_saving_path), exist_ok=True)

best_loss = 1e15

# best_loss, lvae_losses = train_kfold_patch(model_type, path_best_fold_model, folds_index, algo_settings, 
#                                      nb_epochs=100, lr=initial_lr, latent_dimension=latent_representation_size,
#                                      nn_saving_path=LVAE_saving_path, longitudinal_saving_path=longitudinal_saving_path,
#                                      loss_graph_saving_path=f"{output_path}/loss_longitudinal_only", previous_best_loss=best_loss,
#                                      spatial_loss=loss_function, batch_size=batch_size, num_workers=num_worker)


best_loss, lvae_losses = train_kfold_patch_v1(model_type, path_best_fold_model, folds_index, algo_settings, 
                                     nb_epochs=100, lr=initial_lr, latent_dimension=latent_representation_size,
                                     nn_saving_path=LVAE_saving_path, longitudinal_saving_path=longitudinal_saving_path,
                                     loss_graph_saving_path=f"{output_path}/loss_longitudinal_only", previous_best_loss=best_loss,
                                     spatial_loss=loss_function, batch_size=batch_size, num_workers=num_worker)


# best_loss, lvae_losses = train_kfold_patch_v2(model_type, path_best_fold_model, folds_index, algo_settings, 
#                                      nb_epochs=100, lr=initial_lr, latent_dimension=latent_representation_size,
#                                      nn_saving_path=LVAE_saving_path, longitudinal_saving_path=longitudinal_saving_path,
#                                      loss_graph_saving_path=f"{output_path}/loss_longitudinal_only", previous_best_loss=best_loss,
#                                      spatial_loss=loss_function, batch_size=batch_size, num_workers=num_worker)

best_fold_LVAE = CV_LVAE(model_type, folds_index, test_df, LVAE_saving_path, longitudinal_saving_path, 
                         latent_dimension=latent_representation_size, gamma=gamma, beta=beta, iterations=args.iterations,
                         plot_save_path=output_path, cv_patch=True)


save_best_fold_path_VAE = f"saved_models_2D/best_patch_fold_CVAE2D_{args.dimension}_{args.beta}_{args.gamma}_{args.iterations}.pth"
save_best_fold_path_LVAE = f"saved_models_2D/best_patch_fold_longitudinal_estimator_params_CVAE2D_{args.dimension}_{args.beta}_{args.gamma}_{args.iterations}.json"

best_fold_model = model_type(latent_representation_size)
best_fold_model.gamma = gamma
best_fold_model.beta = beta
best_fold_model.load_state_dict(torch.load(LVAE_saving_path+f"_fold_{best_fold_LVAE}.pth2", map_location='cpu'))
torch.save(best_fold_model.state_dict(), save_best_fold_path_VAE+"2")
longitudinal_estimator = Leaspy.load(longitudinal_saving_path+f"_fold_{best_fold_LVAE}.json2")
longitudinal_estimator.save(save_best_fold_path_LVAE+"2")

