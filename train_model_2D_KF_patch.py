import numpy as np
import pandas as pd
import torch
from leaspy import AlgorithmSettings, Leaspy
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import torchvision.transforms as transforms
import argparse
import os

from dataset.Dataset2D import Dataset2D_patch
from longitudinalModel.fit_longitudinal_estimator_on_nn import fit_longitudinal_estimator_on_nn
# from nnModels.CVAE2D import CVAE2D
from longitudinalModel.train import train, train_kfold
from dataset.group_based_train_test_split import group_based_train_test_split
from dataset.split_k_folds import train_k_folds_split_patch

from nnModels.CVAE2D_PATCH import CVAE2D_PATCH
from nnModels.losses import spatial_auto_encoder_loss

from utils_display.display_individual_observations_2D import display_individual_observations_2D
from dataset.LongitudinalDataset2D_patch import LongitudinalDataset2D_patch, longitudinal_collate_2D_patch
from nnModels.train_AE import train_AE_kfold
from cross_validation import CV_VAE, CV_LVAE
"""
Script to train the full model. Neural network model + longitudinal estimator
"""
parser = argparse.ArgumentParser()
parser.add_argument('--data', type=str, required=False, default="./data_csv/starmen_dataset_patch.csv",
                    help='csv file path')
parser.add_argument('--nnmodel_name', type=str, required=False, default='CVAE2D',
                    help='Name of the NN model that will be used')
parser.add_argument('--gamma', type=float, required=False, default=100,
                    help='hyperparameter gamma value used for computing the loss')
parser.add_argument('--beta', type=float, required=False, default=5,
                    help='hyperparameter beta value used for computing the loss, default = 5')
parser.add_argument('--dimension', type=int, required=False, default=3,
                    help='size of the latent representation generated by the neural network encoder, default =4')
parser.add_argument('--iterations', type=int, required=False, default=200,
                    help='Number of iterations when training the longitudinal estimator, default = 200')
parser.add_argument('--lr', type=float, required=False, default=1e-4,
                    help='Learning rate to train the VAE, default = 1e-4')
parser.add_argument('--batch_size', type=int, required=False, default=20,
                    help='batch_size to train the VAE, default = 5')
parser.add_argument('-f', '--freeze', type=str, required=False, default='n',
                    help='freeze convolution layer ? default = n')
parser.add_argument('--dataset', type=str, required=True, default="noacc",
                    help='Use the models trained on dataset "acc" or "noacc"')
temp_args, _ = parser.parse_known_args()

if temp_args.freeze == "y":
    freeze_path = "freeze_conv"
elif temp_args.freeze == "yy":
    freeze_path = "freeze_all"
else:
    freeze_path = "no_freeze"

parser.add_argument('--nnmodel_path', type=str, required=False,
                    default=f'saved_models_2D/dataset_{temp_args.dataset}/{freeze_path}/folds/patch_{temp_args.nnmodel_name}_{temp_args.dimension}_{temp_args.beta}_{temp_args.gamma}_{temp_args.iterations}',
                    help='path where the neural network model parameters are saved')
parser.add_argument('--longitudinal_estimator_path', type=str, required=False,
                    default=f'saved_models_2D/dataset_{temp_args.dataset}/{freeze_path}/folds/patch_longitudinal_estimator_params_{temp_args.nnmodel_name}_{temp_args.dimension}_{temp_args.beta}_{temp_args.gamma}_{temp_args.iterations}',
                    help='path where the longitudinal estimator parameters are saved')
parser.add_argument("-skip", type=str, required=False, default="n")
args = parser.parse_args()

# First we get the different train/validation/test dataset
df = pd.read_csv(args.data)


train_val_df, test_df = group_based_train_test_split(df, test_size=0.2, group_col='subject_id', random_state=42)
test_df.to_csv('data_csv/starmen_patch_test_set.csv', index=False)
folds_index = train_k_folds_split_patch(train_val_df, 8)

###  Hyperparameters of the Variational autoencoder model
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print("device = ", device)
num_worker = round(os.cpu_count()/2)   # For faster GPU training

batch_size = args.batch_size
latent_representation_size = args.dimension
gamma = args.gamma
beta = args.beta
initial_lr = args.lr
nn_saving_path = args.nnmodel_path
longitudinal_saving_path = args.longitudinal_estimator_path

# HERE TO CHANGE VAE ARCHITECTURE
model = CVAE2D_PATCH(latent_representation_size)
model.gamma = gamma
model.beta = beta
loss_function = spatial_auto_encoder_loss
print(f"{args.nnmodel_name}_{beta}_{gamma}_{latent_representation_size}_{args.iterations}")

### Hyperparameters of the longitudinal estimator
all_losses = []
algo_settings = AlgorithmSettings('mcmc_saem', n_iter=args.iterations, seed=45, noise_model="gaussian_diagonal", device=device)

# Preparation of the data
transformations = transforms.Compose([])

def open_npy(path):
    return torch.from_numpy(np.load(path)).float()


output_path = f"training_plots/dataset_{temp_args.dataset}/VAE_patch_folds/patch_{args.nnmodel_name}_{temp_args.dimension}_{temp_args.beta}_{temp_args.gamma}_{args.iterations}/"
os.makedirs(os.path.dirname(output_path), exist_ok=True)

# Training of the vanilla VAE
VAE_saving_path = f"saved_models_2D/dataset_{temp_args.dataset}/VAE_patch_folds/{temp_args.nnmodel_name}_{temp_args.dimension}_{temp_args.beta}_{temp_args.gamma}_{temp_args.iterations}"
os.makedirs(os.path.dirname(f"saved_models_2D/dataset_{temp_args.dataset}/VAE_patch_folds/"), exist_ok=True)
path_best_fold_model = f"saved_models_2D/dataset_{temp_args.dataset}/best_patch_fold_{temp_args.nnmodel_name}_{temp_args.dimension}_{temp_args.beta}_{temp_args.gamma}_{temp_args.iterations}.pth"

if args.skip == "n":
    train_AE_kfold(CVAE2D_PATCH, folds_index, nb_epochs=50, device=device,
                nn_saving_path=VAE_saving_path,
                loss_graph_saving_path=output_path, spatial_loss=loss_function,
                batch_size=batch_size, num_workers=num_worker,
                latent_dimension=latent_representation_size, gamma=gamma, beta=beta, train_patch=True)
    
    best_fold = CV_VAE(CVAE2D_PATCH, folds_index, test_df, VAE_saving_path, temp_args.dataset, plot_save_path=output_path,
                    latent_dimension=latent_representation_size, gamma=gamma, beta=beta,
                    batch_size=batch_size, num_worker=num_worker, cv_patch=True)

    model = CVAE2D_PATCH(latent_representation_size)
    model.gamma = gamma
    model.beta = beta
    model.load_state_dict(torch.load(VAE_saving_path+f"_fold_{best_fold}.pth", map_location='cpu'))
    torch.save(model.state_dict(), path_best_fold_model)

# Training of the Longitudinal VAE
output_path = f"training_plots/dataset_{temp_args.dataset}/{freeze_path}/folds/{args.nnmodel_name}_{temp_args.dimension}_{temp_args.beta}_{temp_args.gamma}_{args.iterations}/"
os.makedirs(os.path.dirname(output_path), exist_ok=True)

if args.freeze == "y":
    freeze = "freeze_conv"
elif args.freeze == "yy":
    freeze = "freeze_all"
else:
    freeze = "no_freeze"

best_loss = 1e15

# validation_dataset = LongitudinalDataset2D_patch('data_csv/starmen_patch_validation_set.csv', read_image=open_npy, transform=transformations)
# easy_dataset = LongitudinalDataset2D_patch('data_csv/starmen_patch_train_set.csv', read_image=open_npy, transform=transformations)

# data_loader = DataLoader(easy_dataset, batch_size=10, num_workers=num_worker, shuffle=False, collate_fn=longitudinal_collate_2D_patch)
# validation_data_loader = DataLoader(validation_dataset, batch_size=10, num_workers=num_worker, shuffle=False, collate_fn=longitudinal_collate_2D_patch)

os.makedirs(os.path.dirname(nn_saving_path), exist_ok=True)
os.makedirs(os.path.dirname(longitudinal_saving_path), exist_ok=True)

best_loss, lvae_losses = train_kfold(CVAE2D_PATCH, path_best_fold_model, folds_index, algo_settings, 
                                     nb_epochs=100, lr=initial_lr, freeze=freeze, latent_dimension=latent_representation_size,
                                     nn_saving_path=nn_saving_path, longitudinal_saving_path=longitudinal_saving_path,
                                     loss_graph_saving_path=f"{output_path}/loss_longitudinal_only", previous_best_loss=best_loss,
                                     spatial_loss=loss_function, batch_size=batch_size, num_workers=num_worker, train_patch=True)

best_fold_LVAE = CV_LVAE(CVAE2D_PATCH, folds_index, test_df, nn_saving_path, longitudinal_saving_path, temp_args.dataset, latent_dimension=latent_representation_size, plot_save_path=output_path, cv_patch=True)


save_best_fold_path_VAE = f"saved_models_2D/dataset_{temp_args.dataset}/{freeze_path}/best_patch_{freeze_path}_fold_CVAE2D_{args.dimension}_{args.beta}_{args.gamma}_{args.iterations}.pth"
save_best_fold_path_LVAE = f"saved_models_2D/dataset_{temp_args.dataset}/{freeze_path}/best_patch_{freeze_path}_fold_longitudinal_estimator_params_CVAE2D_{args.dimension}_{args.beta}_{args.gamma}_{args.iterations}.json"

best_fold_model = CVAE2D_PATCH(latent_representation_size)
best_fold_model.gamma = gamma
best_fold_model.beta = beta
best_fold_model.load_state_dict(torch.load(nn_saving_path+f"_fold_{best_fold_LVAE}.pth2", map_location='cpu'))
torch.save(best_fold_model.state_dict(), save_best_fold_path_VAE+"2")
longitudinal_estimator = Leaspy.load(longitudinal_saving_path+f"_fold_{best_fold_LVAE}.json2")
longitudinal_estimator.save(save_best_fold_path_LVAE+"2")

